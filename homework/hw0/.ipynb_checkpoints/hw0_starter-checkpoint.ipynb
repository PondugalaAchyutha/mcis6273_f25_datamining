{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "440f4ed5-37d8-4248-8bef-8337deb15ce2",
   "metadata": {},
   "source": [
    "# HW0 SOLUTION : [Achyutha Pondugala| 999904194]"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1cff700c-a716-42bc-ae1e-7c2dc0bb5251",
   "metadata": {},
   "source": [
    "### (80%) Perform basic Python coding using Light Pollution Dataset \n",
    "\n",
    "\n",
    "**&#167; Task:**  **Use Python and BeautifulSoup to write code to extract all data files from GaN.**\n",
    "\n",
    "Your _primary_ task is the following:\n",
    "\n",
    "* use BeautifulSoup 4 to find all CSV files from the following URL:\n",
    "  * [https://globeatnight.org/maps-data/](https://globeatnight.org/maps-data/)\n",
    "\n",
    "The subtasks of your code must do the following:\n",
    "\n",
    "  1. extract the page using BeautifulSoup 4,\n",
    "  2. search for the `<li><a>` tags that contain the pattern \n",
    "     `href=\"/documents/*GaN*20*.csv\"`,\n",
    "  3. store the contents of the `href=\"<contents_to_store>\"` patterns in a file, prepending this pattern with `https://globeatnight/` \n",
    "     (e.g. if the `\"<contents_to_store>\"` content is `\"/documents/123/GaN1901.csv\"` \n",
    "     then the stored string is \n",
    "     `https://globeatnight.org/documents/123/GaN1901.csv`),\n",
    "  4. the strings should be on a single line and the file called `data/gan_urls.txt`,\n",
    "  5. use the Python library `os` to make a folder called `data/` where \n",
    "     the data file containing the URLs will be stored."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "4bc303bd-3915-404d-891a-9a3950278fb6",
   "metadata": {},
   "outputs": [],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import os\n",
    "import requests # useful and easy to use but your solution could use urllib or other HTTP libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "495a9794-d2d8-41f3-9ac9-87397c8b6e96",
   "metadata": {},
   "outputs": [],
   "source": [
    "## Your solution goes in this cell -> DO NOT OVER THINK IT, THE SOLUTION CAN BE DONE IN 7-8 LINES OF PYTHON CODE\n",
    "html_link = \"https://globeatnight.org/maps-data/\"\n",
    "extracted_page = requests.get(html_link).content\n",
    "soup = BeautifulSoup(extracted_page, \"html.parser\")\n",
    "\n",
    "# To create folder for our extracted data\n",
    "os.makedirs(\"data\", exist_ok=True) \n",
    "\n",
    "# .CSV file Extraction form <li><a> \n",
    "with open(\"data/gan_urls.txt\", \"w\") as f: # To store the urls in the text file\n",
    "    for li in soup.find_all(\"li\"):\n",
    "        a_inside = li.find(\"a\", href = True)\n",
    "        if a_inside and \"/documents/\" in a_inside[\"href\"] and \"GaN\" in a_inside[\"href\"] and \"20\" in a_inside[\"href\"] and a_inside[\"href\"].endswith(\".csv\"):\n",
    "            f.write(\"https://globeatnight.org\" + a_inside[\"href\"] + \"\\n\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9991837f-c1c5-4132-b09f-11c2527455c4",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:unidata-standard]",
   "language": "python",
   "name": "conda-env-unidata-standard-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.11"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
